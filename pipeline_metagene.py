"""===========================
Pipeline template
===========================

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.yml` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_pipeline_metagene.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_genesets`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
import sys
import os
from ruffus import transform, regex, suffix, follows, subdivide, add_inputs, formatter, merge, split, mkdir
from ruffus.combinatorics import product
from CGATCore import Pipeline as P
from CGATCore import IOTools
from CGAT import GTF
import pandas as pd
import requests
import json

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])


# ------------------------------------------------------------------------------
def get_encode_file(acc):
    encode_url = "https://encodeproject.org"
    headers = {'accept': 'application/json'}

    url = os.path.join(encode_url, "file", acc, "?frame=object")

    response = requests.get(url, headers=headers).json()

    file_location = encode_url + response["href"]
    file_type = response["file_format"].split()[0]

    if file_location.endswith(".gz"):
        file_type += ".gz"
        
    return file_location, file_type


# ------------------------------------------------------------------------------
def process_remote(infile):

    repository, acc = IOTools.open_file(infile).readlines()[0].strip().split()

    if repository == "ENCODE":
        location, filetype = get_encode_file(acc)
    elif repository == "URL":
        location = acc
        if acc.endswith("gz"):
            filetype = ".".join(acc.split(".")[-2])
        else:
            filetype = acc.split(".")[-1]
    else:
        raise ValueError("repository %s not yet supported" % repository)
    
    tmpfile = P.get_temp_filename(shared=False, suffix="."+filetype)
    
    preamble = "wget %(location)s -O %(tmpfile)s --quiet &&" 
    postamble = "&&  rm %(tmpfile)s" 
    
    if filetype == "bam":
        preamble += "samtools index %(tmpfile)s && "
        postamble += " && rm %(tmpfile)s.bai "
    elif filetype == "bed.gz":
        tmp2 = P.get_temp_filename(shared=False)
        preamble += ''' zcat %(tmpfile)s | sort -k1,1 -k2,2n | bgzip > %(tmp2)s &&
                        mv %(tmp2)s %(tmpfile)s &&
                        tabix -p bed %(tmpfile)s && '''
        postamble += "&& rm %(tmpfile)s.tbi"
 
    return preamble % locals(), postamble % locals(), tmpfile, filetype


# ------------------------------------------------------------------------------
@subdivide("*.categories.tsv",
	   regex("(.+).categories.tsv"),
           add_inputs(PARAMS["geneset"]),
           r"\1_*.gtf.gz",
           r"\1")
def split_gtf_by_category(infiles, outfiles, catname):

    catfile, gtffile = infiles
    categories = pd.read_csv(catfile, index_col=0, squeeze=True, sep="\t")
    
    # create output filepool
    outpool = IOTools.FilePool("{}_%s.gtf.gz".format(catname), force=True)

    gtffile = IOTools.open_file(gtffile)

    for gtfline in GTF.iterator(gtffile):

        try:
            transcript_id = gtfline.transcript_id
        except AttributeError:
            transcript_id = None

        try:
            gene_id = gtfline.gene_id
        except AttributeError:
            gene_id = None

        if transcript_id in categories.index:
            outpool.write(categories[transcript_id], str(gtfline) + "\n")
        elif gene_id in categories.index:
            outpool.write(categories[gene_id], str(gtfline) + "\n")

    outpool.close()

    
# ------------------------------------------------------------------------------
@active_if("bam2geneprofile" in PARAMS["methods"])
@product(["*.bam","*.bed.gz","*.bw", "*.remote"],
         formatter(".+/(?P<track>.+)\.(?P<filetype>bam|bed.gz|bw|bed|bigWig|remote)"),
         [split_gtf_by_category, PARAMS["geneset"]],
         formatter(".+/(?P<geneset>.+).gtf.gz"),
         r"bam2geneprofile.dir/{track[0][0]}.vs.{geneset[1][0]}.%s.matrix.tsv.gz" %
           PARAMS["bam2geneprofile_method"],
         r"{filetype[0][0]}")
def do_metagene(infiles, outfile, filetype):
    ''' Compute metagene of input bamfile over genes in geneset.gtf.gz '''
    
    filetype_lookup = {"bam": "--bam-file",
                       "bed": "--bed-file",
                       "bed.gz": "--bed-file",
                       "bw": "--bigwigfile",
                       "bigWig": "--bigwig_file"}
    
    bamfile, gtffile = infiles

    if filetype == "remote":
        preamble, postamble, bamfile, filetype = process_remote(bamfile)
    else:
        preamble, postamble = "", ""
        
    outpattern = P.snip(outfile,
                        ".%s.matrix.tsv.gz" % PARAMS["bam2geneprofile_method"])

    other_options = ""
    if PARAMS["bam2geneprofile"]["options"] is not None:
        for option, value in PARAMS["bam2geneprofile"].get("options", dict()).items():
            if value is True:
                other_options.append(option)
            else:
                other_options.append("{}={}".format((option, value)))
            
    statement = ''' %(preamble)s
                    cgat bam2geneprofile
                        --bam-file=%(bamfile)s
                        --gtf-file=%(gtffile)s
                        --reporter=gene
                        --method=%(bam2geneprofile_method)s
                        --normalize-transcript=%(bam2geneprofile_transcript_normalization)s
                        --normalize-profile=%(bam2geneprofile_profile_normalization)s
                        -P %(outpattern)s.%%s
                        -L %(outpattern)s.log
                        %(other_options)s
                   %(postamble)s'''
    
    P.run(statement)


@active_if("iclip_transcript_region_metagene" in PARAMS["methods"])
@product(["*.bam","*.bed.gz","*.bw", "*.remote"],
         formatter(".+/(?P<track>.+)\.(?P<filetype>bam|bed.gz|bw|bed|bigWig|remote)"),
         [split_gtf_by_category, PARAMS["geneset"]],
         formatter(".+/(?P<geneset>.+).gtf.gz"),
         r"iclip_transcript_regions.dir/{track[0][0]}.vs.{geneset[1][0]}.tsv.gz",
         r"{filetype[0][0]}")
def do_iclip_metagene(infiles, outfile, filetype):

    bamfile, gtffile = infiles

    if filetype == "remote":
        preamble, postamble, bamfile, filetype = process_remote(bamfile)
    else:
        preamble, postamble = "", ""
        
    other_options = ""
    if PARAMS["transcript_regions"]["options"] is not None:
        for option, value in PARAMS["bam2geneprofile"].get("options", dict()).items():
            if value is True:
                other_options.append(option)
            else:
                other_options.append("{}={}".format((option, value)))

    statement=''' %(preamble)s
                   python %(transcript_regions_src_dir)s/scripts/iCLIP_transcript_regions_metagene.dir
                        -I %(gtffile)s
                         %(input)s
                        --regions=%(tran
script_regions_regions)s
                        -S %(outfile)s
                        -L %(outfile)s.log
                        %(other_options)s
                   %(postamble)s'''

    P.run(statement, condaenv=PARAMS["transcript_regions"]["conda_env"])
    
# ------------------------------------------------------------------------------
@merge(do_metagene, "bam2geneprofile.load")
def merge_and_load_metagenes(infiles, outfile):

    P.concatenate_and_load(infiles, outfile,
                           regex_filename=".+/(.+)-(.+)-(.+)\.vs\.(.+)\.([^\.]+)\.matrix.tsv.gz",
                           cat="source,condition,replicate,geneset,method",
                           options=" -i source -i condition -i replicate -i geneset")


@merge(do_iclip_metagene, "transcript_regions.load")
def merge_and_load_region_metagenes(infiles, outfile):

    
    P.concatenate_and_load(infiles, outfile,
                           regex_filename=".+/(.+)-(.+)-(.+)\.vs\.(.+).tsv.gz",
                           cat="source,condition,replicate,geneset",
                           options=" -i source -i condition -i replicate -i geneset")
    
#  -------------------------------------------------------------------------    
@follows(mkdir(os.path.join(PARAMS["export"], "images")))
@split(merge_and_load_metagenes,
       os.path.join(PARAMS["export"], "images/*.%s" % PARAMS["plotting"]["format"]))
def do_plots(infile, outfiles):

    code_location = os.path.dirname(__file__)
    script_file = os.path.join(code_location, "plot_metagenes.R")
    
    statement = "Rscript %(script_file)s"
    P.run(statement)

    
# ---------------------------------------------------
# Generic pipeline tasks
@follows(do_plots, merge_and_load_region_metagenes)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
